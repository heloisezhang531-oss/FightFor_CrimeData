import os
import time
import pandas as pd
from sqlalchemy import create_engine, text
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# --- é…ç½®ä¿¡æ¯ ---
# Local Data Directory
DATA_DIR = r"D:\NUS\IT5006\project\FBI data"

# æ•°æ®åº“é…ç½®
TIDB_USER = os.getenv("TIDB_USER")
TIDB_PASSWORD = os.getenv("TIDB_PASSWORD")
TIDB_HOST = os.getenv("TIDB_HOST")
TIDB_PORT = os.getenv("TIDB_PORT")
TIDB_DB_NAME = os.getenv("TIDB_DB_NAME")
CA_PATH = os.getenv("TID_CA_PATH")

# æ„å»ºè¿æ¥å­—ç¬¦ä¸²
conn_str = f"mysql+pymysql://{TIDB_USER}:{TIDB_PASSWORD}@{TIDB_HOST}:{TIDB_PORT}/{TIDB_DB_NAME}?ssl_ca={CA_PATH}"
engine = create_engine(conn_str)

# NIBRS è¡¨åˆ—è¡¨ (æ ¹æ® NIBRS æ•°æ®ç»“æ„)
NIBRS_TABLES = [
    "nibrs_weapon",
    "nibrs_criminal_act",
    "nibrs_victim_offense",
    "nibrs_victim_offender_rel",
    "nibrs_victim_injury",
    "nibrs_victim_circumstances",
    "nibrs_victim",
    "nibrs_suspected_drug",
    "nibrs_suspect_using",
    "nibrs_property_desc",
    "nibrs_property",
    "nibrs_offense",
    "nibrs_offender",
    "nibrs_incident",
    "nibrs_month",
    "nibrs_bias_motivation",
    "nibrs_arrestee_weapon",
    "nibrs_arrestee",
    "cde_agencies",
    "agency_participation",
    "nibrs_age",
    "nibrs_arrest_type",
    "nibrs_assignment_type",
    "nibrs_bias_list",
    "nibrs_circumstances",
    "nibrs_cleared_except",
    "nibrs_criminal_act_type",
    "nibrs_drug_measure",
    "nibrs_drug_measure_type",
    "nibrs_ethnicity",
    "nibrs_injury",
    "nibrs_justifiable_force",
    "nibrs_location_type",
    "nibrs_offense_type",
    "nibrs_prop_desc_type",
    "nibrs_prop_loss_type",
    "nibrs_relationship",
    "nibrs_suspected_drug_type",
    "nibrs_using_list",
    "nibrs_victim_type",
    "nibrs_weapon_type"
]

from sqlalchemy import create_engine, text, inspect

# ... (imports remain the same)

def process_and_upload_local_data():
    """
    éå†æœ¬åœ°æ–‡ä»¶å¤¹ (IL-2015 åˆ° IL-2024)ï¼Œè¯»å– CSV æ–‡ä»¶å¹¶ä¸Šä¼ åˆ° TiDBã€‚
    """
    inspector = inspect(engine)

    # éå†å¹´ä»½
    for year in range(2015, 2025):
        year_folder_name = f"IL-{year}"
        year_dir_path = os.path.join(DATA_DIR, year_folder_name)
        
        print(f"\nğŸš€ --- å¤„ç†å¹´ä»½: {year} (æ–‡ä»¶å¤¹: {year_folder_name}) ---")
        
        if not os.path.exists(year_dir_path):
             print(f"    âš ï¸ æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {year_dir_path}ï¼Œè·³è¿‡è¯¥å¹´ä»½ã€‚")
             continue

        # éå†æ¯å¼ è¡¨
        for table in NIBRS_TABLES:
            csv_filename = f"{table}.csv"
            csv_file_path = os.path.join(year_dir_path, csv_filename)
            
            print(f"  ğŸ“‚ å¤„ç†è¡¨: {table} (æ–‡ä»¶: {csv_filename})")
            
            if not os.path.exists(csv_file_path):
                print(f"    âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {csv_file_path}ï¼Œè·³è¿‡ã€‚")
                continue
            
            # Check table existence via Inspector
            table_exists = False
            db_columns = []
            try:
                # Refresh inspector implies just calling checks, but inspector object might cache? 
                # Safer to verify existence directly or rely on engine.
                if inspector.has_table(table):
                    table_exists = True
                    # Get columns
                    columns_info = inspector.get_columns(table)
                    db_columns = [col['name'] for col in columns_info]
                    # print(f"    ğŸ“‹ Table found. Columns: {db_columns}")
                else:
                    # print(f"    ğŸ†• Table {table} does not exist. Will create.")
                    table_exists = False
            except Exception as e:
                print(f"    âš ï¸ Error checking table {table}: {e}")
                # Fallback to assuming not exists or connection issue
                table_exists = False

            try:
                # 1. è¯»å– CSV æ•°æ®
                chunk_size = 10000
                total_records = 0
                
                if os.path.getsize(csv_file_path) < 100:
                     df_peek = pd.read_csv(csv_file_path, nrows=1)
                     if df_peek.empty:
                         print(f"    âš ï¸ æ–‡ä»¶ä¸ºç©ºæˆ–æ— æ•°æ®ï¼Œè·³è¿‡ã€‚")
                         continue

                for chunk_idx, df in enumerate(pd.read_csv(csv_file_path, chunksize=chunk_size)):
                    
                    # 2. æ¸…æ´—æ•°æ®
                    # ç»Ÿä¸€è½¬å°å†™
                    df.columns = [col.lower() for col in df.columns]
                    
                    # ç¡®ä¿æœ‰ä¸€ä¸ª data_year å­—æ®µ
                    if 'data_year' not in df.columns:
                        df['data_year'] = year

                    df_final = df
                    
                    if table_exists:
                        # è¿‡æ»¤åˆ—ï¼šåªä¿ç•™ DB ä¸­å­˜åœ¨çš„åˆ— (ä¸¥æ ¼æ¨¡å¼)
                        valid_columns = []
                        db_col_set = set([c.lower() for c in db_columns])
                        
                        for col in df.columns:
                            if col.lower() in db_col_set:
                                valid_columns.append(col)
                        
                        df_final = df[valid_columns].copy()
                        
                        # DEBUG
                        if chunk_idx == 0:
                            dropped = set(df.columns) - set(valid_columns)
                            if dropped:
                                print(f"    â„¹ï¸ (è¡¨å·²å­˜åœ¨) ä¸¢å¼ƒ CSV ä¸­å¤šä½™çš„åˆ—: {dropped}")
                    else:
                        if chunk_idx == 0:
                            print(f"    ğŸ†• (è¡¨ä¸å­˜åœ¨) å‡†å¤‡æ•°æ®ç”¨äºæ–°å»ºè¡¨...")

                     # å¤„ç†å¤æ‚å­—æ®µ
                    for col in df_final.columns:
                        if df_final[col].dtype == 'object':
                             df_final[col] = df_final[col].apply(lambda x: str(x) if isinstance(x, (list, dict)) else x)

                    # 3. å†™å…¥ TiDB
                    start_time = time.time()
                    try:
                        # if_exists='append': works for both new (creates) and existing.
                        df_final.to_sql(table, engine, if_exists='append', index=False, chunksize=1000)
                        cost = time.time() - start_time
                        
                        records_count = len(df_final)
                        total_records += records_count
                        print(f"    ğŸ’¾ Chunk {chunk_idx+1}: resource saved {records_count} records ({cost:.2f}s)")
                        
                    except Exception as sql_err:
                        print(f"    âŒ å†™å…¥æ•°æ®åº“å¤±è´¥ (Chunk {chunk_idx+1}): {sql_err}")
                        break

                print(f"    âœ… {table} {year} å®Œæˆï¼Œå…±å¤„ç† {total_records} æ¡è®°å½•ã€‚")

            except Exception as e:
                print(f"    âŒ è¯»å–æˆ–å¤„ç†æ–‡ä»¶å¤±è´¥: {e}")
                continue

if __name__ == "__main__":
    if not os.path.exists(DATA_DIR):
         print(f"âŒ é”™è¯¯: æ•°æ®ç›®å½•ä¸å­˜åœ¨ -> {DATA_DIR}")
         print("è¯·ç¡®è®¤è·¯å¾„æ˜¯å¦æ­£ç¡®ã€‚")
    else:
        process_and_upload_local_data()